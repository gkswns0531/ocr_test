diff --git a/glmocr/layout/layout_detector.py b/glmocr/layout/layout_detector.py
index 0a551f2..96837df 100644
--- a/glmocr/layout/layout_detector.py
+++ b/glmocr/layout/layout_detector.py
@@ -6,6 +6,7 @@ from __future__ import annotations
 from pathlib import Path
 from typing import TYPE_CHECKING, List, Dict, Optional
 
+import cv2
 import torch
 import numpy as np
 from PIL import Image
@@ -66,7 +67,9 @@ class PPDocLayoutDetector(BaseLayoutDetector):
         self._model = PPDocLayoutV3ForObjectDetection.from_pretrained(self.model_dir)
         self._model.eval()
 
-        if torch.cuda.is_available():
+        if str(self.cuda_visible_devices).lower() == "cpu":
+            self._device = "cpu"
+        elif torch.cuda.is_available():
             self._device = (
                 f"cuda:{self.cuda_visible_devices}"
                 if self.cuda_visible_devices is not None
@@ -210,12 +213,48 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 raw_results.append(self._empty_detection_result())
         return raw_results
 
+    def _gpu_preprocess_from_paths(
+        self, image_paths: List[str], target_h: int, target_w: int
+    ) -> tuple:
+        """Fast image decode + GPU resize. Returns (pixel_values, original_sizes).
+
+        JPEG: nvJPEG hardware decode on GPU (~20x faster than PIL).
+        PNG/other: cv2 CPU decode + GPU resize (still faster than PIL+processor).
+        """
+        import torchvision.io
+        import torch.nn.functional as F
+
+        original_sizes = []
+        all_tensors = []
+        for path in image_paths:
+            if path.lower().endswith(('.jpg', '.jpeg')):
+                # GPU hardware JPEG decode via nvJPEG
+                with open(path, 'rb') as f:
+                    data = f.read()
+                jt = torch.frombuffer(bytearray(data), dtype=torch.uint8)
+                decoded = torchvision.io.decode_jpeg(jt, device=self._device)  # (3, H, W) uint8
+            else:
+                # CPU decode for PNG/other formats, then transfer to GPU
+                img = cv2.imread(path, cv2.IMREAD_COLOR)
+                img_rgb = img[:, :, ::-1].copy()  # BGR → RGB
+                decoded = torch.from_numpy(img_rgb).permute(2, 0, 1).to(self._device)  # (3, H, W) uint8
+            _, h, w = decoded.shape
+            original_sizes.append((w, h))  # (width, height)
+            resized = F.interpolate(
+                decoded.unsqueeze(0).float(), size=(target_h, target_w),
+                mode='bilinear', align_corners=False,
+            )
+            all_tensors.append(resized.squeeze(0))
+        pixel_values = torch.stack(all_tensors).div_(255.0)
+        return pixel_values, original_sizes
+
     def process(
         self,
         images: List[Image.Image],
         save_visualization: bool = False,
         visualization_output_dir: Optional[str] = None,
         global_start_idx: int = 0,
+        image_paths: Optional[List[str]] = None,
     ) -> List[List[Dict]]:
         """Batch-detect layout regions in-process.
 
@@ -224,6 +263,7 @@ class PPDocLayoutDetector(BaseLayoutDetector):
             save_visualization: Whether to also save visualization.
             visualization_output_dir: Where to save visualization outputs.
             global_start_idx: Start index for visualization filenames (layout_page{N}).
+            image_paths: Optional file paths for fast decode (JPEG: GPU nvJPEG, PNG: cv2).
 
         Returns:
             List[List[Dict]]: Detection results per image.
@@ -231,28 +271,77 @@ class PPDocLayoutDetector(BaseLayoutDetector):
         if self._model is None:
             raise RuntimeError("Layout detector not started. Call start() first.")
 
+        import time as _time
+        _t0 = _time.time()
+
+        _proc_size = getattr(self._image_processor, 'size', None)
+        _target_h = _proc_size.get('height', 800) if isinstance(_proc_size, dict) else 800
+        _target_w = _proc_size.get('width', 800) if isinstance(_proc_size, dict) else 800
         num_images = len(images)
-        image_batch = []
-        for image in images:
-            image_width, image_height = image.size
-            image_array = np.array(image.convert("RGB"))
-            image_batch.append((image_array, image_width, image_height))
 
-        pil_images = [Image.fromarray(img[0]) for img in image_batch]
+        # Fast path: decode from file paths (JPEG on GPU, PNG via cv2)
+        _use_gpu_decode = (
+            image_paths is not None
+            and len(image_paths) == num_images
+            and self._device.startswith("cuda")
+        )
+
         all_paddle_format_results = []
 
         for chunk_start in range(0, num_images, self.batch_size):
             chunk_end = min(chunk_start + self.batch_size, num_images)
-            chunk_pil = pil_images[chunk_start:chunk_end]
+            _tc0 = _time.time()
 
-            inputs = self._image_processor(images=chunk_pil, return_tensors="pt")
-            inputs = {k: v.to(self._device) for k, v in inputs.items()}
+            if _use_gpu_decode:
+                chunk_paths = image_paths[chunk_start:chunk_end]
+                pixel_values, chunk_orig_sizes = self._gpu_preprocess_from_paths(
+                    chunk_paths, _target_h, _target_w,
+                )
+                if chunk_start == 0:
+                    original_sizes = list(chunk_orig_sizes)
+                else:
+                    original_sizes.extend(chunk_orig_sizes)
+            else:
+                # CPU fallback: cv2 resize
+                if chunk_start == 0:
+                    original_sizes = []
+                    resized_arrays = []
+                    for image in images:
+                        img = image if image.mode == "RGB" else image.convert("RGB")
+                        original_sizes.append(img.size)
+                        arr = np.asarray(img)
+                        if arr.shape[0] != _target_h or arr.shape[1] != _target_w:
+                            arr = cv2.resize(arr, (_target_w, _target_h), interpolation=cv2.INTER_LINEAR)
+                        resized_arrays.append(arr)
+                chunk_arrays = resized_arrays[chunk_start:chunk_end]
+                batch_np = np.stack(chunk_arrays)
+                pixel_values = (
+                    torch.from_numpy(batch_np)
+                    .permute(0, 3, 1, 2)
+                    .contiguous()
+                    .to(self._device, non_blocking=True)
+                    .float()
+                    .div_(255.0)
+                )
+                chunk_orig_sizes = original_sizes[chunk_start:chunk_end]
+
+            inputs = {"pixel_values": pixel_values}
+            if self._device.startswith("cuda"):
+                torch.cuda.synchronize()
+            _tc1 = _time.time()
+            logger.info(f"Layout process: preprocess {chunk_end - chunk_start} imgs: {_tc1 - _tc0:.2f}s" +
+                        (" [GPU decode]" if _use_gpu_decode else " [CPU cv2]"))
 
             with torch.no_grad():
                 outputs = self._model(**inputs)
+            if self._device.startswith("cuda"):
+                torch.cuda.synchronize()
+            _tc2 = _time.time()
+            logger.info(f"Layout process: model inference: {_tc2 - _tc1:.2f}s")
 
+            # Use original image sizes for bbox mapping (not resized)
             target_sizes = torch.tensor(
-                [img.size[::-1] for img in chunk_pil], device=self._device
+                [(h, w) for w, h in chunk_orig_sizes], device=self._device
             )
             try:
                 if hasattr(outputs, "pred_boxes") and outputs.pred_boxes is not None:
@@ -285,13 +374,25 @@ class PPDocLayoutDetector(BaseLayoutDetector):
             else:
                 pre_threshold = self.threshold
 
+            _tc3 = _time.time()
+            # Create PIL images for fallback post-processing path
+            if _use_gpu_decode:
+                # Convert GPU tensors back to PIL for fallback (rare path)
+                chunk_pil_for_fallback = [
+                    Image.fromarray((pixel_values[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))
+                    for i in range(chunk_end - chunk_start)
+                ]
+            else:
+                chunk_pil_for_fallback = [Image.fromarray(a) for a in chunk_arrays]
             raw_results = self._post_process_chunk_with_fallback(
-                chunk_pil, outputs, target_sizes, pre_threshold, chunk_start
+                chunk_pil_for_fallback, outputs, target_sizes, pre_threshold, chunk_start
             )
+            _tc4 = _time.time()
+            logger.info(f"Layout process: post_process_object_detection: {_tc4 - _tc3:.2f}s")
 
             if self.threshold_by_class:
                 raw_results = self._apply_per_class_threshold(raw_results)
-            img_sizes = [img.size for img in chunk_pil]
+            img_sizes = list(chunk_orig_sizes)
             paddle_format_results = apply_layout_postprocess(
                 raw_results=raw_results,
                 id2label=self.id2label,
@@ -300,18 +401,22 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 layout_unclip_ratio=self.layout_unclip_ratio,
                 layout_merge_bboxes_mode=self.layout_merge_bboxes_mode,
             )
+            _tc5 = _time.time()
+            logger.info(f"Layout process: apply_layout_postprocess: {_tc5 - _tc4:.2f}s")
             all_paddle_format_results.extend(paddle_format_results)
 
             if self._device.startswith("cuda") and chunk_end < num_images:
                 del inputs, outputs, raw_results
                 torch.cuda.empty_cache()
 
+        _t_vis0 = _time.time()
         saved_vis_paths = []
         if save_visualization and visualization_output_dir:
             vis_output_path = Path(visualization_output_dir)
             vis_output_path.mkdir(parents=True, exist_ok=True)
             for img_idx, img_results in enumerate(all_paddle_format_results):
-                vis_img = np.array(pil_images[img_idx])
+                img = images[img_idx]
+                vis_img = np.asarray(img if img.mode == "RGB" else img.convert("RGB"))
                 save_filename = f"layout_page{global_start_idx + img_idx}.jpg"
                 save_path = vis_output_path / save_filename
                 save_layout_visualization(
@@ -324,10 +429,13 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 )
                 saved_vis_paths.append(str(save_path))
 
+        _t_vis1 = _time.time()
+        logger.info(f"Layout process: visualization: {_t_vis1 - _t_vis0:.2f}s")
+
         all_results = []
+        _t_norm0 = _time.time()
         for img_idx, paddle_results in enumerate(all_paddle_format_results):
-            image_width = image_batch[img_idx][1]
-            image_height = image_batch[img_idx][2]
+            image_width, image_height = original_sizes[img_idx]
             results = []
             valid_index = 0
             for item in paddle_results:
@@ -370,4 +478,7 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 valid_index += 1
             all_results.append(results)
 
+        _t_norm1 = _time.time()
+        logger.info(f"Layout process: normalization: {_t_norm1 - _t_norm0:.2f}s")
+        logger.info(f"Layout process: TOTAL: {_t_norm1 - _t0:.2f}s for {num_images} images")
         return all_results
diff --git a/glmocr/pipeline/pipeline.py b/glmocr/pipeline/pipeline.py
index eeeb7b5..d6205b9 100644
--- a/glmocr/pipeline/pipeline.py
+++ b/glmocr/pipeline/pipeline.py
@@ -11,11 +11,13 @@ Extension options:
 from __future__ import annotations
 
 import queue
+import time
 import threading
-from dataclasses import dataclass
+from dataclasses import dataclass, field
 from typing import TYPE_CHECKING, Dict, Any, Optional, Tuple, List, Generator
 from concurrent.futures import ThreadPoolExecutor, as_completed
 
+from PIL import Image
 from glmocr.dataloader import PageLoader
 from glmocr.ocr_client import OCRClient
 from glmocr.parser_result import PipelineResult
@@ -32,6 +34,23 @@ logger = get_logger(__name__)
 profiler = get_profiler(__name__)
 
 
+@dataclass
+class PipelineTimings:
+    """Timing data for pipeline profiling."""
+    pipeline_start: float = 0.0
+    data_loading_start: float = 0.0
+    data_loading_end: float = 0.0
+    layout_start: float = 0.0
+    layout_end: float = 0.0
+    layout_batch_times: list = field(default_factory=list)  # list of (n_images, seconds)
+    vlm_start: float = 0.0
+    vlm_end: float = 0.0
+    vlm_request_times: list = field(default_factory=list)  # list of seconds per request
+    n_regions_total: int = 0
+    n_regions_skipped: int = 0
+    first_result_time: float = 0.0
+
+
 @dataclass
 class _AsyncPipelineState:
     """Shared state for the 3-thread layout path (loader -> layout -> recognition)."""
@@ -42,6 +61,7 @@ class _AsyncPipelineState:
     recognition_results: List[Tuple[int, Dict]]
     results_lock: threading.Lock
     images_dict: Dict[int, Any]
+    image_paths_dict: Dict[int, str]  # img_idx → JPEG file path for GPU decode
     layout_results_dict: Dict[int, List]
     num_images_loaded: List[int]
     unit_indices_holder: List[Optional[List[int]]]
@@ -51,6 +71,7 @@ class _AsyncPipelineState:
     count_lock: threading.Lock
     exceptions: List[Tuple[str, Exception]]
     exception_lock: threading.Lock
+    timings: PipelineTimings = field(default_factory=PipelineTimings)
 
 
 class Pipeline:
@@ -135,6 +156,7 @@ class Pipeline:
             recognition_results=[],
             results_lock=threading.Lock(),
             images_dict={},
+            image_paths_dict={},
             layout_results_dict={},
             num_images_loaded=[0],
             unit_indices_holder=[None],
@@ -144,6 +166,7 @@ class Pipeline:
             count_lock=threading.Lock(),
             exceptions=[],
             exception_lock=threading.Lock(),
+            timings=PipelineTimings(pipeline_start=time.time()),
         )
 
     def process(
@@ -303,19 +326,31 @@ class Pipeline:
 
         state = self._create_async_pipeline_state(page_maxsize, region_maxsize)
 
+        # Build mapping from unit_idx to file path for fast layout preprocessing & crop
+        _url_to_path = {}
+        for unit_idx, url in enumerate(image_urls):
+            path = url[7:] if url.startswith("file://") else url
+            if path.lower().endswith(('.jpg', '.jpeg', '.png')) and __import__('os').path.isfile(path):
+                _url_to_path[unit_idx] = path
+
         def data_loading_thread() -> None:
             try:
+                state.timings.data_loading_start = time.time()
                 img_idx = 0
                 unit_indices_list: List[int] = []
                 for page, unit_idx in self.page_loader.iter_pages_with_unit_indices(
                     image_urls
                 ):
                     state.images_dict[img_idx] = page
+                    # Store image path for fast layout preprocessing & crop
+                    if unit_idx in _url_to_path:
+                        state.image_paths_dict[img_idx] = _url_to_path[unit_idx]
                     state.page_queue.put(("image", img_idx, page))
                     unit_indices_list.append(unit_idx)
                     img_idx += 1
                     state.num_images_loaded[0] = img_idx
                     state.unit_indices_holder[0] = list(unit_indices_list)
+                state.timings.data_loading_end = time.time()
                 state.page_queue.put(("done", None, None))
             except Exception as e:
                 logger.exception("Data loading thread error: %s", e)
@@ -327,6 +362,7 @@ class Pipeline:
 
         def layout_detection_thread() -> None:
             try:
+                state.timings.layout_start = time.time()
                 batch_images: List[Any] = []
                 batch_indices: List[int] = []
                 loading_complete = False
@@ -345,6 +381,7 @@ class Pipeline:
                                 save_layout_visualization,
                                 layout_vis_output_dir,
                                 global_start_idx,
+                                _state=state,
                             )
                             global_start_idx += len(batch_indices)
                             batch_images = []
@@ -363,6 +400,7 @@ class Pipeline:
                                 save_layout_visualization,
                                 layout_vis_output_dir,
                                 global_start_idx,
+                                _state=state,
                             )
                             global_start_idx += len(batch_indices)
                             batch_images = []
@@ -379,7 +417,9 @@ class Pipeline:
                                 save_layout_visualization,
                                 layout_vis_output_dir,
                                 global_start_idx,
+                                _state=state,
                             )
+                        state.timings.layout_end = time.time()
                         state.region_queue.put(("done", None, None))
                         break
                     elif item_type == "error":
@@ -434,7 +474,8 @@ class Pipeline:
 
         def vlm_recognition_thread() -> None:
             try:
-                executor = ThreadPoolExecutor(max_workers=min(self.max_workers, 128))
+                state.timings.vlm_start = time.time()
+                executor = ThreadPoolExecutor(max_workers=self.max_workers)
                 futures: Dict[Any, Tuple[Dict, str, int]] = {}
                 pending_skip: List[Tuple[Dict, str, int]] = []
                 processing_complete = False
@@ -506,7 +547,9 @@ class Pipeline:
                             state.recognition_results.append((page_idx, info))
                         maybe_notify_ready_units(page_idx)
                 executor.shutdown(wait=True)
+                state.timings.vlm_end = time.time()
             except Exception as e:
+                state.timings.vlm_end = time.time()
                 logger.exception("VLM recognition thread error: %s", e)
                 with state.exception_lock:
                     state.exceptions.append(("VLMRecognitionThread", e))
@@ -605,6 +648,17 @@ class Pipeline:
             emitted.add(u)
 
         t3.join()
+        # Expose timing data
+        self.last_timings = state.timings
+        t = state.timings
+        logger.info(
+            "Pipeline timings: data_load=%.1fs, layout=%.1fs (batches=%s), vlm=%.1fs, total=%.1fs",
+            t.data_loading_end - t.data_loading_start if t.data_loading_end else 0,
+            t.layout_end - t.layout_start if t.layout_end else 0,
+            [(n, f"{s:.1f}s") for n, s in t.layout_batch_times],
+            t.vlm_end - t.vlm_start if t.vlm_end else 0,
+            time.time() - t.pipeline_start,
+        )
         with state.exception_lock:
             if state.exceptions:
                 raise RuntimeError("; ".join(f"{n}: {e}" for n, e in state.exceptions))
@@ -619,20 +673,67 @@ class Pipeline:
         save_visualization: bool,
         vis_output_dir: Optional[str],
         global_start_idx: int,
+        _state: Optional[_AsyncPipelineState] = None,
     ) -> None:
         """Run layout detection on a batch and push regions to queue2."""
+        _t0 = time.time()
+        # Collect image paths for fast preprocessing (JPEG: GPU nvJPEG, PNG: cv2)
+        image_paths = None
+        if _state is not None and _state.image_paths_dict:
+            paths = [_state.image_paths_dict.get(idx) for idx in batch_indices]
+            if all(p is not None for p in paths):
+                image_paths = paths
         layout_results = self.layout_detector.process(
             batch_images,
             save_visualization=save_visualization and vis_output_dir is not None,
             visualization_output_dir=vis_output_dir,
             global_start_idx=global_start_idx,
+            image_paths=image_paths,
         )
+        # Pre-convert images to numpy once to avoid repeated np.asarray in crop.
+        # Use cv2.imread for JPEG files (~3x faster than PIL lazy decode).
+        import numpy as _np
+        import cv2 as _cv2
+        _np_cache = {}
         for img_idx, image, layout_result in zip(
             batch_indices, batch_images, layout_results
         ):
             layout_results_dict[img_idx] = layout_result
+            if layout_result:
+                if img_idx not in _np_cache:
+                    _path = _state.image_paths_dict.get(img_idx) if _state else None
+                    if _path:
+                        _np_cache[img_idx] = _cv2.imread(_path, _cv2.IMREAD_COLOR)[:, :, ::-1].copy()
+                    else:
+                        _np_cache[img_idx] = _np.asarray(image)
+                img_array = _np_cache[img_idx]
+                img_h, img_w = img_array.shape[:2]
             for region in layout_result:
-                cropped = crop_image_region(image, region["bbox_2d"], region["polygon"])
+                # Inline fast crop (avoids per-crop np.asarray overhead)
+                x1_n, y1_n, x2_n, y2_n = region["bbox_2d"]
+                x1 = int(x1_n * img_w / 1000)
+                y1 = int(y1_n * img_h / 1000)
+                x2 = int(x2_n * img_w / 1000)
+                y2 = int(y2_n * img_h / 1000)
+                polygon = region.get("polygon")
+                img_crop = img_array[y1:y2, x1:x2]
+                crop_h, crop_w = img_crop.shape[:2] if img_crop.size > 0 else (0, 0)
+                if crop_h > 0 and crop_w > 0 and polygon and len(polygon) >= 3:
+                    scale_x = img_w / 1000
+                    scale_y = img_h / 1000
+                    poly_px = _np.array(
+                        [[int(p[0] * scale_x) - x1, int(p[1] * scale_y) - y1] for p in polygon],
+                        dtype=_np.int32,
+                    )
+                    mask = _np.zeros((crop_h, crop_w), dtype=_np.uint8)
+                    _cv2.fillPoly(mask, [poly_px], 1)
+                    output = _np.full_like(img_crop, 255, dtype=_np.uint8)
+                    _cv2.copyTo(img_crop, mask, output)
+                    cropped = Image.fromarray(output)
+                elif crop_h > 0 and crop_w > 0:
+                    cropped = Image.fromarray(img_crop.copy())
+                else:
+                    cropped = crop_image_region(image, region["bbox_2d"], region.get("polygon"))
                 region_queue.put(
                     (
                         "region",
@@ -640,6 +741,13 @@ class Pipeline:
                         (cropped, region, region["task_type"], img_idx),
                     )
                 )
+        del _np_cache
+        _elapsed = time.time() - _t0
+        if _state is not None:
+            _state.timings.layout_batch_times.append((len(batch_images), _elapsed))
+        logger.info("Layout batch: %d images in %.2fs (%.1f img/s)",
+                     len(batch_images), _elapsed,
+                     len(batch_images) / _elapsed if _elapsed > 0 else 0)
 
     def _extract_image_urls(self, request_data: Dict[str, Any]) -> List[str]:
         """Extract image URLs from request_data."""
