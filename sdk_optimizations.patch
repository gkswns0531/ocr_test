diff --git a/glmocr/dataloader/page_loader.py b/glmocr/dataloader/page_loader.py
index 61362af..85ade39 100644
--- a/glmocr/dataloader/page_loader.py
+++ b/glmocr/dataloader/page_loader.py
@@ -340,30 +340,35 @@ class PageLoader:
         if image.mode != "RGB":
             image = image.convert("RGB")
 
-        # Encode image
-        buffered = BytesIO()
-        image.save(buffered, format=self.image_format)
-        img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
+        # Encode directly (single pass: smart_resize → JPEG → base64).
+        # Previous code did JPEG+base64 here, then _process_msg_standard decoded
+        # it back to PIL and re-encoded — a full redundant encode/decode cycle.
+        encoded_image = load_image_to_base64(
+            image,
+            t_patch_size=self.t_patch_size,
+            max_pixels=self.max_pixels,
+            image_format=self.image_format,
+            patch_expand_factor=self.patch_expand_factor,
+            min_pixels=self.min_pixels,
+        )
 
-        original_msg = {
+        msg = {
             "role": "user",
             "content": [
                 {
                     "type": "image_url",
                     "image_url": {
-                        "url": f"data:image/{self.image_format.lower()};base64,{img_base64}"
+                        "url": f"data:image/{self.image_format.lower()};base64,{encoded_image}"
                     },
                 },
             ],
         }
 
         if prompt_text:
-            original_msg["content"].append({"type": "text", "text": prompt_text})
-
-        processed_msg = self._process_msg_standard(original_msg)
+            msg["content"].append({"type": "text", "text": prompt_text})
 
         return {
-            "messages": [processed_msg],
+            "messages": [msg],
             "max_tokens": self.max_tokens,
             "temperature": self.temperature,
             "top_p": self.top_p,
diff --git a/glmocr/layout/layout_detector.py b/glmocr/layout/layout_detector.py
index 0a551f2..30bf311 100644
--- a/glmocr/layout/layout_detector.py
+++ b/glmocr/layout/layout_detector.py
@@ -6,6 +6,7 @@ from __future__ import annotations
 from pathlib import Path
 from typing import TYPE_CHECKING, List, Dict, Optional
 
+import cv2
 import torch
 import numpy as np
 from PIL import Image
@@ -66,7 +67,9 @@ class PPDocLayoutDetector(BaseLayoutDetector):
         self._model = PPDocLayoutV3ForObjectDetection.from_pretrained(self.model_dir)
         self._model.eval()
 
-        if torch.cuda.is_available():
+        if str(self.cuda_visible_devices).lower() == "cpu":
+            self._device = "cpu"
+        elif torch.cuda.is_available():
             self._device = (
                 f"cuda:{self.cuda_visible_devices}"
                 if self.cuda_visible_devices is not None
@@ -74,10 +77,26 @@ class PPDocLayoutDetector(BaseLayoutDetector):
             )
         else:
             self._device = "cpu"
+
+        # BF16 inference: ~2x throughput on Ampere+ GPUs (saves memory too)
+        if self._device.startswith("cuda") and torch.cuda.is_bf16_supported():
+            self._model = self._model.bfloat16()
+            logger.debug("Layout model cast to bfloat16")
+
         self._model = self._model.to(self._device)
+        self._model_dtype = next(self._model.parameters()).dtype
+
+        # torch.compile: graph-level operator fusion
+        if self._device.startswith("cuda"):
+            try:
+                self._model = torch.compile(self._model)
+                logger.debug("torch.compile enabled for layout model")
+            except Exception as e:
+                logger.warning("torch.compile failed, using eager mode: %s", e)
+
         if self.id2label is None:
             self.id2label = self._model.config.id2label
-        logger.debug(f"PP-DocLayoutV3 loaded on device: {self._device}")
+        logger.debug(f"PP-DocLayoutV3 loaded on {self._device} (dtype={self._model_dtype})")
 
     def stop(self):
         """Unload model and processor."""
@@ -166,9 +185,10 @@ class PPDocLayoutDetector(BaseLayoutDetector):
     ) -> Dict:
         """Run model + post_process for a single image. Raises on error."""
         single_inputs = self._image_processor(images=[image], return_tensors="pt")
-        single_inputs = {k: v.to(self._device) for k, v in single_inputs.items()}
-        with torch.no_grad():
+        single_inputs = {k: v.to(self._device, dtype=self._model_dtype) for k, v in single_inputs.items()}
+        with torch.inference_mode():
             single_outputs = self._model(**single_inputs)
+        self._outputs_to_float32(single_outputs)
         single_target = torch.tensor([image.size[::-1]], device=self._device)
         single_raw = self._image_processor.post_process_object_detection(
             single_outputs,
@@ -177,6 +197,14 @@ class PPDocLayoutDetector(BaseLayoutDetector):
         )
         return single_raw[0]
 
+    @staticmethod
+    def _outputs_to_float32(outputs) -> None:
+        """Cast model outputs to float32 in-place for post_process compatibility."""
+        for attr in ("logits", "pred_boxes", "pred_masks", "out_masks"):
+            t = getattr(outputs, attr, None)
+            if t is not None and t.dtype != torch.float32:
+                setattr(outputs, attr, t.float())
+
     def _post_process_chunk_with_fallback(
         self,
         chunk_pil: List[Image.Image],
@@ -186,6 +214,8 @@ class PPDocLayoutDetector(BaseLayoutDetector):
         chunk_start: int,
     ) -> List[Dict]:
         """Run batch post_process; on failure, retry image-by-image."""
+        # HuggingFace post_process_object_detection requires FP32
+        self._outputs_to_float32(outputs)
         try:
             return self._image_processor.post_process_object_detection(
                 outputs,
@@ -210,12 +240,51 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 raw_results.append(self._empty_detection_result())
         return raw_results
 
+    def _gpu_preprocess_from_paths(
+        self, image_paths: List[str], target_h: int, target_w: int
+    ) -> tuple:
+        """Fast image decode + GPU resize. Returns (pixel_values, original_sizes).
+
+        JPEG: nvJPEG hardware decode on GPU (~20x faster than PIL).
+        PNG/other: cv2 CPU decode + GPU resize (still faster than PIL+processor).
+        Outputs in model dtype (BF16 on Ampere+, FP32 otherwise).
+        """
+        import torchvision.io
+        import torch.nn.functional as F
+
+        _dtype = self._model_dtype  # bfloat16 or float32
+
+        original_sizes = []
+        all_tensors = []
+        for path in image_paths:
+            if path.lower().endswith(('.jpg', '.jpeg')):
+                # GPU hardware JPEG decode via nvJPEG
+                with open(path, 'rb') as f:
+                    data = f.read()
+                jt = torch.frombuffer(bytearray(data), dtype=torch.uint8)
+                decoded = torchvision.io.decode_jpeg(jt, device=self._device)  # (3, H, W) uint8
+            else:
+                # CPU decode for PNG/other formats, then transfer to GPU
+                img = cv2.imread(path, cv2.IMREAD_COLOR)
+                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
+                decoded = torch.from_numpy(img_rgb).permute(2, 0, 1).to(self._device)  # (3, H, W) uint8
+            _, h, w = decoded.shape
+            original_sizes.append((w, h))  # (width, height)
+            resized = F.interpolate(
+                decoded.unsqueeze(0).to(dtype=_dtype), size=(target_h, target_w),
+                mode='bilinear', align_corners=False,
+            )
+            all_tensors.append(resized.squeeze(0))
+        pixel_values = torch.stack(all_tensors).div_(255.0)
+        return pixel_values, original_sizes
+
     def process(
         self,
         images: List[Image.Image],
         save_visualization: bool = False,
         visualization_output_dir: Optional[str] = None,
         global_start_idx: int = 0,
+        image_paths: Optional[List[str]] = None,
     ) -> List[List[Dict]]:
         """Batch-detect layout regions in-process.
 
@@ -224,6 +293,7 @@ class PPDocLayoutDetector(BaseLayoutDetector):
             save_visualization: Whether to also save visualization.
             visualization_output_dir: Where to save visualization outputs.
             global_start_idx: Start index for visualization filenames (layout_page{N}).
+            image_paths: Optional file paths for fast decode (JPEG: GPU nvJPEG, PNG: cv2).
 
         Returns:
             List[List[Dict]]: Detection results per image.
@@ -231,28 +301,76 @@ class PPDocLayoutDetector(BaseLayoutDetector):
         if self._model is None:
             raise RuntimeError("Layout detector not started. Call start() first.")
 
+        import time as _time
+        _t0 = _time.time()
+
+        _proc_size = getattr(self._image_processor, 'size', None)
+        _target_h = _proc_size.get('height', 800) if isinstance(_proc_size, dict) else 800
+        _target_w = _proc_size.get('width', 800) if isinstance(_proc_size, dict) else 800
         num_images = len(images)
-        image_batch = []
-        for image in images:
-            image_width, image_height = image.size
-            image_array = np.array(image.convert("RGB"))
-            image_batch.append((image_array, image_width, image_height))
 
-        pil_images = [Image.fromarray(img[0]) for img in image_batch]
+        # Fast path: decode from file paths (JPEG on GPU, PNG via cv2)
+        _use_gpu_decode = (
+            image_paths is not None
+            and len(image_paths) == num_images
+            and self._device.startswith("cuda")
+        )
+
         all_paddle_format_results = []
 
         for chunk_start in range(0, num_images, self.batch_size):
             chunk_end = min(chunk_start + self.batch_size, num_images)
-            chunk_pil = pil_images[chunk_start:chunk_end]
+            _tc0 = _time.time()
 
-            inputs = self._image_processor(images=chunk_pil, return_tensors="pt")
-            inputs = {k: v.to(self._device) for k, v in inputs.items()}
+            if _use_gpu_decode:
+                chunk_paths = image_paths[chunk_start:chunk_end]
+                pixel_values, chunk_orig_sizes = self._gpu_preprocess_from_paths(
+                    chunk_paths, _target_h, _target_w,
+                )
+                if chunk_start == 0:
+                    original_sizes = list(chunk_orig_sizes)
+                else:
+                    original_sizes.extend(chunk_orig_sizes)
+            else:
+                # CPU fallback: cv2 resize
+                if chunk_start == 0:
+                    original_sizes = []
+                    resized_arrays = []
+                    for image in images:
+                        img = image if image.mode == "RGB" else image.convert("RGB")
+                        original_sizes.append(img.size)
+                        arr = np.asarray(img)
+                        if arr.shape[0] != _target_h or arr.shape[1] != _target_w:
+                            arr = cv2.resize(arr, (_target_w, _target_h), interpolation=cv2.INTER_LINEAR)
+                        resized_arrays.append(arr)
+                chunk_arrays = resized_arrays[chunk_start:chunk_end]
+                batch_np = np.stack(chunk_arrays)
+                pixel_values = (
+                    torch.from_numpy(batch_np)
+                    .permute(0, 3, 1, 2)
+                    .contiguous()
+                    .to(self._device, dtype=self._model_dtype, non_blocking=True)
+                    .div_(255.0)
+                )
+                chunk_orig_sizes = original_sizes[chunk_start:chunk_end]
 
-            with torch.no_grad():
+            inputs = {"pixel_values": pixel_values}
+            if self._device.startswith("cuda"):
+                torch.cuda.synchronize()
+            _tc1 = _time.time()
+            logger.info(f"Layout process: preprocess {chunk_end - chunk_start} imgs: {_tc1 - _tc0:.2f}s" +
+                        (" [GPU decode]" if _use_gpu_decode else " [CPU cv2]"))
+
+            with torch.inference_mode():
                 outputs = self._model(**inputs)
+            if self._device.startswith("cuda"):
+                torch.cuda.synchronize()
+            _tc2 = _time.time()
+            logger.info(f"Layout process: model inference: {_tc2 - _tc1:.2f}s")
 
+            # Use original image sizes for bbox mapping (not resized)
             target_sizes = torch.tensor(
-                [img.size[::-1] for img in chunk_pil], device=self._device
+                [(h, w) for w, h in chunk_orig_sizes], device=self._device
             )
             try:
                 if hasattr(outputs, "pred_boxes") and outputs.pred_boxes is not None:
@@ -285,13 +403,25 @@ class PPDocLayoutDetector(BaseLayoutDetector):
             else:
                 pre_threshold = self.threshold
 
+            _tc3 = _time.time()
+            # Create PIL images for fallback post-processing path
+            if _use_gpu_decode:
+                # Convert GPU tensors back to PIL for fallback (rare path)
+                chunk_pil_for_fallback = [
+                    Image.fromarray((pixel_values[i].float().permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))
+                    for i in range(chunk_end - chunk_start)
+                ]
+            else:
+                chunk_pil_for_fallback = [Image.fromarray(a) for a in chunk_arrays]
             raw_results = self._post_process_chunk_with_fallback(
-                chunk_pil, outputs, target_sizes, pre_threshold, chunk_start
+                chunk_pil_for_fallback, outputs, target_sizes, pre_threshold, chunk_start
             )
+            _tc4 = _time.time()
+            logger.info(f"Layout process: post_process_object_detection: {_tc4 - _tc3:.2f}s")
 
             if self.threshold_by_class:
                 raw_results = self._apply_per_class_threshold(raw_results)
-            img_sizes = [img.size for img in chunk_pil]
+            img_sizes = list(chunk_orig_sizes)
             paddle_format_results = apply_layout_postprocess(
                 raw_results=raw_results,
                 id2label=self.id2label,
@@ -300,18 +430,22 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 layout_unclip_ratio=self.layout_unclip_ratio,
                 layout_merge_bboxes_mode=self.layout_merge_bboxes_mode,
             )
+            _tc5 = _time.time()
+            logger.info(f"Layout process: apply_layout_postprocess: {_tc5 - _tc4:.2f}s")
             all_paddle_format_results.extend(paddle_format_results)
 
             if self._device.startswith("cuda") and chunk_end < num_images:
                 del inputs, outputs, raw_results
                 torch.cuda.empty_cache()
 
+        _t_vis0 = _time.time()
         saved_vis_paths = []
         if save_visualization and visualization_output_dir:
             vis_output_path = Path(visualization_output_dir)
             vis_output_path.mkdir(parents=True, exist_ok=True)
             for img_idx, img_results in enumerate(all_paddle_format_results):
-                vis_img = np.array(pil_images[img_idx])
+                img = images[img_idx]
+                vis_img = np.asarray(img if img.mode == "RGB" else img.convert("RGB"))
                 save_filename = f"layout_page{global_start_idx + img_idx}.jpg"
                 save_path = vis_output_path / save_filename
                 save_layout_visualization(
@@ -324,10 +458,13 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 )
                 saved_vis_paths.append(str(save_path))
 
+        _t_vis1 = _time.time()
+        logger.info(f"Layout process: visualization: {_t_vis1 - _t_vis0:.2f}s")
+
         all_results = []
+        _t_norm0 = _time.time()
         for img_idx, paddle_results in enumerate(all_paddle_format_results):
-            image_width = image_batch[img_idx][1]
-            image_height = image_batch[img_idx][2]
+            image_width, image_height = original_sizes[img_idx]
             results = []
             valid_index = 0
             for item in paddle_results:
@@ -370,4 +507,7 @@ class PPDocLayoutDetector(BaseLayoutDetector):
                 valid_index += 1
             all_results.append(results)
 
+        _t_norm1 = _time.time()
+        logger.info(f"Layout process: normalization: {_t_norm1 - _t_norm0:.2f}s")
+        logger.info(f"Layout process: TOTAL: {_t_norm1 - _t0:.2f}s for {num_images} images")
         return all_results
diff --git a/glmocr/pipeline/pipeline.py b/glmocr/pipeline/pipeline.py
index eeeb7b5..1326450 100644
--- a/glmocr/pipeline/pipeline.py
+++ b/glmocr/pipeline/pipeline.py
@@ -11,11 +11,13 @@ Extension options:
 from __future__ import annotations
 
 import queue
+import time
 import threading
-from dataclasses import dataclass
+from dataclasses import dataclass, field
 from typing import TYPE_CHECKING, Dict, Any, Optional, Tuple, List, Generator
 from concurrent.futures import ThreadPoolExecutor, as_completed
 
+from PIL import Image
 from glmocr.dataloader import PageLoader
 from glmocr.ocr_client import OCRClient
 from glmocr.parser_result import PipelineResult
@@ -32,6 +34,29 @@ logger = get_logger(__name__)
 profiler = get_profiler(__name__)
 
 
+def _build_and_process(page_loader, ocr_client, image, task_type):
+    """Build VLM request and call API (runs inside ThreadPoolExecutor)."""
+    req = page_loader.build_request_from_image(image, task_type)
+    return ocr_client.process(req)
+
+
+@dataclass
+class PipelineTimings:
+    """Timing data for pipeline profiling."""
+    pipeline_start: float = 0.0
+    data_loading_start: float = 0.0
+    data_loading_end: float = 0.0
+    layout_start: float = 0.0
+    layout_end: float = 0.0
+    layout_batch_times: list = field(default_factory=list)  # list of (n_images, seconds)
+    vlm_start: float = 0.0
+    vlm_end: float = 0.0
+    vlm_request_times: list = field(default_factory=list)  # list of seconds per request
+    n_regions_total: int = 0
+    n_regions_skipped: int = 0
+    first_result_time: float = 0.0
+
+
 @dataclass
 class _AsyncPipelineState:
     """Shared state for the 3-thread layout path (loader -> layout -> recognition)."""
@@ -42,6 +67,7 @@ class _AsyncPipelineState:
     recognition_results: List[Tuple[int, Dict]]
     results_lock: threading.Lock
     images_dict: Dict[int, Any]
+    image_paths_dict: Dict[int, str]  # img_idx → JPEG file path for GPU decode
     layout_results_dict: Dict[int, List]
     num_images_loaded: List[int]
     unit_indices_holder: List[Optional[List[int]]]
@@ -51,6 +77,7 @@ class _AsyncPipelineState:
     count_lock: threading.Lock
     exceptions: List[Tuple[str, Exception]]
     exception_lock: threading.Lock
+    timings: PipelineTimings = field(default_factory=PipelineTimings)
 
 
 class Pipeline:
@@ -135,6 +162,7 @@ class Pipeline:
             recognition_results=[],
             results_lock=threading.Lock(),
             images_dict={},
+            image_paths_dict={},
             layout_results_dict={},
             num_images_loaded=[0],
             unit_indices_holder=[None],
@@ -144,6 +172,7 @@ class Pipeline:
             count_lock=threading.Lock(),
             exceptions=[],
             exception_lock=threading.Lock(),
+            timings=PipelineTimings(pipeline_start=time.time()),
         )
 
     def process(
@@ -303,19 +332,31 @@ class Pipeline:
 
         state = self._create_async_pipeline_state(page_maxsize, region_maxsize)
 
+        # Build mapping from unit_idx to file path for fast layout preprocessing & crop
+        _url_to_path = {}
+        for unit_idx, url in enumerate(image_urls):
+            path = url[7:] if url.startswith("file://") else url
+            if path.lower().endswith(('.jpg', '.jpeg', '.png')) and __import__('os').path.isfile(path):
+                _url_to_path[unit_idx] = path
+
         def data_loading_thread() -> None:
             try:
+                state.timings.data_loading_start = time.time()
                 img_idx = 0
                 unit_indices_list: List[int] = []
                 for page, unit_idx in self.page_loader.iter_pages_with_unit_indices(
                     image_urls
                 ):
                     state.images_dict[img_idx] = page
+                    # Store image path for fast layout preprocessing & crop
+                    if unit_idx in _url_to_path:
+                        state.image_paths_dict[img_idx] = _url_to_path[unit_idx]
                     state.page_queue.put(("image", img_idx, page))
                     unit_indices_list.append(unit_idx)
                     img_idx += 1
                     state.num_images_loaded[0] = img_idx
                     state.unit_indices_holder[0] = list(unit_indices_list)
+                state.timings.data_loading_end = time.time()
                 state.page_queue.put(("done", None, None))
             except Exception as e:
                 logger.exception("Data loading thread error: %s", e)
@@ -327,6 +368,7 @@ class Pipeline:
 
         def layout_detection_thread() -> None:
             try:
+                state.timings.layout_start = time.time()
                 batch_images: List[Any] = []
                 batch_indices: List[int] = []
                 loading_complete = False
@@ -345,6 +387,7 @@ class Pipeline:
                                 save_layout_visualization,
                                 layout_vis_output_dir,
                                 global_start_idx,
+                                _state=state,
                             )
                             global_start_idx += len(batch_indices)
                             batch_images = []
@@ -363,6 +406,7 @@ class Pipeline:
                                 save_layout_visualization,
                                 layout_vis_output_dir,
                                 global_start_idx,
+                                _state=state,
                             )
                             global_start_idx += len(batch_indices)
                             batch_images = []
@@ -379,7 +423,9 @@ class Pipeline:
                                 save_layout_visualization,
                                 layout_vis_output_dir,
                                 global_start_idx,
+                                _state=state,
                             )
+                        state.timings.layout_end = time.time()
                         state.region_queue.put(("done", None, None))
                         break
                     elif item_type == "error":
@@ -434,7 +480,8 @@ class Pipeline:
 
         def vlm_recognition_thread() -> None:
             try:
-                executor = ThreadPoolExecutor(max_workers=min(self.max_workers, 128))
+                state.timings.vlm_start = time.time()
+                executor = ThreadPoolExecutor(max_workers=self.max_workers)
                 futures: Dict[Any, Tuple[Dict, str, int]] = {}
                 pending_skip: List[Tuple[Dict, str, int]] = []
                 processing_complete = False
@@ -479,10 +526,13 @@ class Pipeline:
                         if task_type == "skip":
                             pending_skip.append((region, task_type, page_idx))
                         else:
-                            req = self.page_loader.build_request_from_image(
-                                cropped_image, task_type
+                            # Parallelize both request building (JPEG encode + base64)
+                            # and API call together in the thread pool.
+                            future = executor.submit(
+                                _build_and_process,
+                                self.page_loader, self.ocr_client,
+                                cropped_image, task_type,
                             )
-                            future = executor.submit(self.ocr_client.process, req)
                             futures[future] = (region, task_type, page_idx)
                     elif item_type == "done":
                         processing_complete = True
@@ -506,7 +556,9 @@ class Pipeline:
                             state.recognition_results.append((page_idx, info))
                         maybe_notify_ready_units(page_idx)
                 executor.shutdown(wait=True)
+                state.timings.vlm_end = time.time()
             except Exception as e:
+                state.timings.vlm_end = time.time()
                 logger.exception("VLM recognition thread error: %s", e)
                 with state.exception_lock:
                     state.exceptions.append(("VLMRecognitionThread", e))
@@ -605,6 +657,17 @@ class Pipeline:
             emitted.add(u)
 
         t3.join()
+        # Expose timing data
+        self.last_timings = state.timings
+        t = state.timings
+        logger.info(
+            "Pipeline timings: data_load=%.1fs, layout=%.1fs (batches=%s), vlm=%.1fs, total=%.1fs",
+            t.data_loading_end - t.data_loading_start if t.data_loading_end else 0,
+            t.layout_end - t.layout_start if t.layout_end else 0,
+            [(n, f"{s:.1f}s") for n, s in t.layout_batch_times],
+            t.vlm_end - t.vlm_start if t.vlm_end else 0,
+            time.time() - t.pipeline_start,
+        )
         with state.exception_lock:
             if state.exceptions:
                 raise RuntimeError("; ".join(f"{n}: {e}" for n, e in state.exceptions))
@@ -619,20 +682,67 @@ class Pipeline:
         save_visualization: bool,
         vis_output_dir: Optional[str],
         global_start_idx: int,
+        _state: Optional[_AsyncPipelineState] = None,
     ) -> None:
         """Run layout detection on a batch and push regions to queue2."""
+        _t0 = time.time()
+        # Collect image paths for fast preprocessing (JPEG: GPU nvJPEG, PNG: cv2)
+        image_paths = None
+        if _state is not None and _state.image_paths_dict:
+            paths = [_state.image_paths_dict.get(idx) for idx in batch_indices]
+            if all(p is not None for p in paths):
+                image_paths = paths
         layout_results = self.layout_detector.process(
             batch_images,
             save_visualization=save_visualization and vis_output_dir is not None,
             visualization_output_dir=vis_output_dir,
             global_start_idx=global_start_idx,
+            image_paths=image_paths,
         )
+        # Pre-convert images to numpy once to avoid repeated np.asarray in crop.
+        # Use cv2.imread for JPEG files (~3x faster than PIL lazy decode).
+        import numpy as _np
+        import cv2 as _cv2
+        _np_cache = {}
         for img_idx, image, layout_result in zip(
             batch_indices, batch_images, layout_results
         ):
             layout_results_dict[img_idx] = layout_result
+            if layout_result:
+                if img_idx not in _np_cache:
+                    _path = _state.image_paths_dict.get(img_idx) if _state else None
+                    if _path:
+                        _np_cache[img_idx] = _cv2.cvtColor(_cv2.imread(_path, _cv2.IMREAD_COLOR), _cv2.COLOR_BGR2RGB)
+                    else:
+                        _np_cache[img_idx] = _np.asarray(image)
+                img_array = _np_cache[img_idx]
+                img_h, img_w = img_array.shape[:2]
             for region in layout_result:
-                cropped = crop_image_region(image, region["bbox_2d"], region["polygon"])
+                # Inline fast crop (avoids per-crop np.asarray overhead)
+                x1_n, y1_n, x2_n, y2_n = region["bbox_2d"]
+                x1 = int(x1_n * img_w / 1000)
+                y1 = int(y1_n * img_h / 1000)
+                x2 = int(x2_n * img_w / 1000)
+                y2 = int(y2_n * img_h / 1000)
+                polygon = region.get("polygon")
+                img_crop = img_array[y1:y2, x1:x2]
+                crop_h, crop_w = img_crop.shape[:2] if img_crop.size > 0 else (0, 0)
+                if crop_h > 0 and crop_w > 0 and polygon and len(polygon) >= 3:
+                    scale_x = img_w / 1000
+                    scale_y = img_h / 1000
+                    poly_px = _np.array(
+                        [[int(p[0] * scale_x) - x1, int(p[1] * scale_y) - y1] for p in polygon],
+                        dtype=_np.int32,
+                    )
+                    mask = _np.zeros((crop_h, crop_w), dtype=_np.uint8)
+                    _cv2.fillPoly(mask, [poly_px], 1)
+                    output = _np.full_like(img_crop, 255, dtype=_np.uint8)
+                    _cv2.copyTo(img_crop, mask, output)
+                    cropped = Image.fromarray(output)
+                elif crop_h > 0 and crop_w > 0:
+                    cropped = Image.fromarray(img_crop.copy())
+                else:
+                    cropped = crop_image_region(image, region["bbox_2d"], region.get("polygon"))
                 region_queue.put(
                     (
                         "region",
@@ -640,6 +750,13 @@ class Pipeline:
                         (cropped, region, region["task_type"], img_idx),
                     )
                 )
+        del _np_cache
+        _elapsed = time.time() - _t0
+        if _state is not None:
+            _state.timings.layout_batch_times.append((len(batch_images), _elapsed))
+        logger.info("Layout batch: %d images in %.2fs (%.1f img/s)",
+                     len(batch_images), _elapsed,
+                     len(batch_images) / _elapsed if _elapsed > 0 else 0)
 
     def _extract_image_urls(self, request_data: Dict[str, Any]) -> List[str]:
         """Extract image URLs from request_data."""
diff --git a/glmocr/utils/image_utils.py b/glmocr/utils/image_utils.py
index e228cbb..187a6ce 100644
--- a/glmocr/utils/image_utils.py
+++ b/glmocr/utils/image_utils.py
@@ -161,13 +161,13 @@ def load_image_to_base64(
         max_pixels=max_pixels,
     )
 
-    # Resize
-    image = image.resize((w_bar, h_bar), Image.Resampling.BICUBIC)
+    # Resize (skip if dimensions already match)
+    if (w_bar, h_bar) != (w, h):
+        image = image.resize((w_bar, h_bar), Image.Resampling.BICUBIC)
 
     # Encode as bytes
     buffered = io.BytesIO()
     image.save(buffered, format=image_format)
-    buffered.seek(0)
     image_data = buffered.getvalue()
 
     # Convert bytes to base64
diff --git a/glmocr/utils/layout_postprocess_utils.py b/glmocr/utils/layout_postprocess_utils.py
index 65e9801..9105251 100644
--- a/glmocr/utils/layout_postprocess_utils.py
+++ b/glmocr/utils/layout_postprocess_utils.py
@@ -29,36 +29,57 @@ def iou(box1, box2):
 
 
 def nms(boxes, iou_same=0.6, iou_diff=0.95):
-    """Perform Non-Maximum Suppression (NMS) with different IoU thresholds for same and different classes."""
-    # Extract class scores
+    """Perform NMS with different IoU thresholds for same and different classes.
+
+    Uses vectorized IoU matrix computation (numpy broadcasting) instead of
+    per-pair scalar Python calls. The greedy selection loop remains sequential
+    but operates on the pre-computed matrix.
+    """
+    n = len(boxes)
+    if n == 0:
+        return []
+
+    classes = boxes[:, 0]
     scores = boxes[:, 1]
+    coords = boxes[:, 2:6]
+
+    # Pre-compute full IoU matrix using broadcasting: O(n²) vectorized
+    x1 = coords[:, 0:1]  # (n, 1)
+    y1 = coords[:, 1:2]
+    x2 = coords[:, 2:3]
+    y2 = coords[:, 3:4]
+
+    areas = (x2 - x1 + 1) * (y2 - y1 + 1)  # (n, 1)
+
+    # Pairwise intersection: (n, 1) op (1, n) → (n, n)
+    xi1 = np.maximum(x1, coords[:, 0])
+    yi1 = np.maximum(y1, coords[:, 1])
+    xi2 = np.minimum(x2, coords[:, 2])
+    yi2 = np.minimum(y2, coords[:, 3])
 
-    # Sort indices by scores in descending order
-    indices = np.argsort(scores)[::-1]
-    selected_boxes = []
+    inter = np.maximum(0, xi2 - xi1 + 1) * np.maximum(0, yi2 - yi1 + 1)
+    union = areas + areas.T - inter
+    iou_matrix = np.where(union > 0, inter / union, 0.0)
 
-    while len(indices) > 0:
-        current = indices[0]
-        current_box = boxes[current]
-        current_class = current_box[0]
-        current_coords = current_box[2:]
+    # Class-aware threshold matrix: (n, n)
+    same_class = (classes[:, None] == classes[None, :])
+    threshold_matrix = np.where(same_class, iou_same, iou_diff)
 
-        selected_boxes.append(current)
-        indices = indices[1:]
+    # Greedy NMS using pre-computed IoU matrix
+    order = np.argsort(scores)[::-1]
+    suppressed = np.zeros(n, dtype=bool)
+    selected = []
 
-        filtered_indices = []
-        for i in indices:
-            box = boxes[i]
-            box_class = box[0]
-            box_coords = box[2:]
-            iou_value = iou(current_coords, box_coords)
-            threshold = iou_same if current_class == box_class else iou_diff
+    for idx in order:
+        if suppressed[idx]:
+            continue
+        selected.append(idx)
+        # Suppress boxes exceeding their class-aware threshold
+        suppress_mask = iou_matrix[idx] >= threshold_matrix[idx]
+        suppressed |= suppress_mask
+        suppressed[idx] = False  # Don't suppress self
 
-            # If the IoU is below the threshold, keep the box
-            if iou_value < threshold:
-                filtered_indices.append(i)
-        indices = filtered_indices
-    return selected_boxes
+    return selected
 
 
 def is_contained(box1, box2):
@@ -80,36 +101,61 @@ def is_contained(box1, box2):
 def check_containment(boxes, preserve_indices=None, category_index=None, mode=None):
     """Check containment relationships among boxes.
 
+    Vectorized using numpy broadcasting: computes (n, n) containment matrix
+    in a single operation instead of O(n²) Python loops.
+
     Args:
-        boxes: Array of boxes
-        preserve_indices: Set of class indices to always preserve (e.g., image, seal, chart)
+        boxes: Array of boxes [cls_id, score, x1, y1, x2, y2]
+        preserve_indices: Set of class indices to always preserve
         category_index: Category index for mode-specific filtering
         mode: Filtering mode ('large' or 'small')
     """
     n = len(boxes)
-    contains_other = np.zeros(n, dtype=int)
-    contained_by_other = np.zeros(n, dtype=int)
+    if n == 0:
+        return np.zeros(0, dtype=int), np.zeros(0, dtype=int)
+
+    classes = boxes[:, 0]
+    x1 = boxes[:, 2:3]  # (n, 1)
+    y1 = boxes[:, 3:4]
+    x2 = boxes[:, 4:5]
+    y2 = boxes[:, 5:6]
+
+    areas = (x2 - x1) * (y2 - y1)  # (n, 1)
+
+    # Pairwise intersection: (n, 1) op (1, n) → (n, n)
+    xi1 = np.maximum(x1, boxes[:, 2])
+    yi1 = np.maximum(y1, boxes[:, 3])
+    xi2 = np.minimum(x2, boxes[:, 4])
+    yi2 = np.minimum(y2, boxes[:, 5])
+
+    inter = np.maximum(0, xi2 - xi1) * np.maximum(0, yi2 - yi1)
+
+    # containment[i, j] = intersection(i,j) / area(i): how much of box_i overlaps box_j
+    containment = np.where(areas > 0, inter / areas, 0.0)
+
+    # Exclude self-containment
+    np.fill_diagonal(containment, 0.0)
+
+    # Preserved boxes should never be marked as contained
+    if preserve_indices is not None and len(preserve_indices) > 0:
+        preserve_mask = np.isin(classes, list(preserve_indices))
+        containment[preserve_mask, :] = 0.0
+
+    # Mode-specific filtering
+    if category_index is not None and mode is not None:
+        if mode == "large":
+            # Only check containment BY the specific category (j must be category)
+            non_cat_j = (classes != category_index)
+            containment[:, non_cat_j] = 0.0
+        elif mode == "small":
+            # Only check containment OF the specific category (i must be category)
+            non_cat_i = (classes != category_index)
+            containment[non_cat_i, :] = 0.0
+
+    is_contained_mask = containment >= 0.8
+    contained_by_other = is_contained_mask.any(axis=1).astype(int)
+    contains_other = is_contained_mask.any(axis=0).astype(int)
 
-    for i in range(n):
-        for j in range(n):
-            if i == j:
-                continue
-            # Skip if box i is in preserve list (should never be marked as contained)
-            if preserve_indices is not None and boxes[i][0] in preserve_indices:
-                continue
-            if category_index is not None and mode is not None:
-                if mode == "large" and boxes[j][0] == category_index:
-                    if is_contained(boxes[i], boxes[j]):
-                        contained_by_other[i] = 1
-                        contains_other[j] = 1
-                if mode == "small" and boxes[i][0] == category_index:
-                    if is_contained(boxes[i], boxes[j]):
-                        contained_by_other[i] = 1
-                        contains_other[j] = 1
-            else:
-                if is_contained(boxes[i], boxes[j]):
-                    contained_by_other[i] = 1
-                    contains_other[j] = 1
     return contains_other, contained_by_other
 
 
@@ -218,20 +264,19 @@ def apply_layout_postprocess(
         polygon_points = result.get("polygon_points", [])
         img_size = img_sizes[img_idx]  # (width, height)
 
-        # Build intermediate format: [cls_id, score, x1, y1, x2, y2, order]
-        boxes_with_order = []
-        for i in range(len(scores)):
-            cls_id = int(labels[i])
-            score = float(scores[i])
-            x1, y1, x2, y2 = boxes[i]
-            order = int(order_seq[i])
-            boxes_with_order.append([cls_id, score, x1, y1, x2, y2, order])
-
-        if len(boxes_with_order) == 0:
+        # Build intermediate format: [cls_id, score, x1, y1, x2, y2, order, orig_idx]
+        # orig_idx tracks the original detection index for O(1) polygon lookup later.
+        n_det = len(scores)
+        if n_det == 0:
             paddle_format_results.append([])
             continue
 
-        boxes_array = np.array(boxes_with_order)
+        boxes_array = np.empty((n_det, 8), dtype=np.float64)
+        boxes_array[:, 0] = labels
+        boxes_array[:, 1] = scores
+        boxes_array[:, 2:6] = boxes
+        boxes_array[:, 6] = order_seq
+        boxes_array[:, 7] = np.arange(n_det)
 
         # Apply NMS
         if layout_nms:
@@ -247,21 +292,16 @@ def apply_layout_postprocess(
                 area_thres = 0.93
             image_index = all_labels.index("image") if "image" in all_labels else None
             img_area = img_size[0] * img_size[1]
-            filtered_boxes = []
-            for box in boxes_array:
-                label_index, score, xmin, ymin, xmax, ymax = box[:6]
-                if label_index == image_index:
-                    xmin = max(0, xmin)
-                    ymin = max(0, ymin)
-                    xmax = min(img_size[0], xmax)
-                    ymax = min(img_size[1], ymax)
-                    box_area = (xmax - xmin) * (ymax - ymin)
-                    if box_area <= area_thres * img_area:
-                        filtered_boxes.append(box)
-                else:
-                    filtered_boxes.append(box)
-            if len(filtered_boxes) > 0:
-                boxes_array = np.array(filtered_boxes)
+            if image_index is not None:
+                is_image = boxes_array[:, 0] == image_index
+                clipped_x1 = np.maximum(0, boxes_array[:, 2])
+                clipped_y1 = np.maximum(0, boxes_array[:, 3])
+                clipped_x2 = np.minimum(img_size[0], boxes_array[:, 4])
+                clipped_y2 = np.minimum(img_size[1], boxes_array[:, 5])
+                box_areas = (clipped_x2 - clipped_x1) * (clipped_y2 - clipped_y1)
+                keep = ~is_image | (box_areas <= area_thres * img_area)
+                if keep.any():
+                    boxes_array = boxes_array[keep]
 
         # Apply merge_bboxes_mode
         if layout_merge_bboxes_mode:
@@ -368,18 +408,13 @@ def apply_layout_postprocess(
             if x1 >= x2 or y1 >= y2:
                 continue
 
-            # Since we may have filtered boxes, we need to match by coordinates
+            # O(1) polygon lookup using tracked orig_idx (column 7)
             poly = None
-            if len(polygon_points) > 0:
-                # Try to find matching polygon by comparing coordinates
-                for orig_idx in range(len(boxes)):
-                    if np.allclose(boxes[orig_idx], box_data[2:6], atol=1.0):
-                        if orig_idx < len(polygon_points):
-                            candidate_poly = polygon_points[orig_idx]
-                            # Some detectors may return None for missing polygons
-                            if candidate_poly is not None:
-                                poly = candidate_poly.astype(np.float32)
-                        break
+            orig_idx = int(box_data[7])
+            if orig_idx < len(polygon_points):
+                candidate_poly = polygon_points[orig_idx]
+                if candidate_poly is not None:
+                    poly = candidate_poly.astype(np.float32)
 
             if poly is None:
                 # Fallback: convert box to 4-point polygon
